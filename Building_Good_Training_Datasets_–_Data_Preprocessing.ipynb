{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "lB0touiM5jxP",
        "-Pr1vM_W5qh_",
        "33fHxRkA9W6u",
        "uaGU7CQjPltE"
      ],
      "authorship_tag": "ABX9TyNvsZJYvMjb6/GnXgW2aFzC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aftabgazali/Building_Good_Training_Datasets_-_Data_Preprocessing.ipynb/blob/main/Building_Good_Training_Datasets_%E2%80%93_Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "OPR1Uo6_-3wf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxxISOQR5QLY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from io import StringIO\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas profiling"
      ],
      "metadata": {
        "id": "9nMESvhqPLDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Missing Data"
      ],
      "metadata": {
        "id": "lB0touiM5jxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a csv file and fill the dummy data with two missing values\n",
        "csv_data = \\\n",
        "'''A,B,C,D\n",
        " 1.0,2.0,3.0,4.0\n",
        "5.0,6.0,,8.0\n",
        "10.0,11.0,12.0,'''\n",
        "\n",
        "df = pd.read_csv(StringIO(csv_data))"
      ],
      "metadata": {
        "id": "N-J70lU_5W59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "YtiKasQk5cdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the count of null values in each column"
      ],
      "metadata": {
        "id": "-Pr1vM_W5qh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "q5i3D0eB5mgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imputing Missing Values"
      ],
      "metadata": {
        "id": "33fHxRkA9W6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Missing values will be replaced by a mean of that feature**\n",
        "\n",
        "***Note: Other strategy includes median or most_frequent, where\n",
        "the latter replaces the missing values with the most frequent values***"
      ],
      "metadata": {
        "id": "TKDtSnqW95_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import Starred\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imr = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imr = imr.fit(df.values)\n",
        "\n",
        "new_df = pd.DataFrame(imr.transform(df.values))\n",
        "new_df.values"
      ],
      "metadata": {
        "id": "DDURaBND9ZXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Same steps but more efficient and easy**"
      ],
      "metadata": {
        "id": "fH0GXy6P-RY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.fillna(df.mean())"
      ],
      "metadata": {
        "id": "8l9cYjBs929Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Categorical Features"
      ],
      "metadata": {
        "id": "FjR4h_Wv-zB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*When we are talking about categorical data, we have to further distinguish between ordinal and nom-\n",
        "inal features. Ordinal features can be understood as categorical values that can be sorted or ordered.\n",
        "For example, t-shirt size would be an ordinal feature, because we can define an **order: XL > L > M.** In\n",
        "contrast, nominal features don’t imply any order; to continue with the previous example, we could\n",
        "think of t-shirt color as a nominal feature since it typically doesn’t make sense to say that, for example,\n",
        "red is larger than blue*"
      ],
      "metadata": {
        "id": "zn0OsQP8_1JF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tshirts = [\n",
        "           ['green', 'M', 10.1,'class_1'],\n",
        "           ['red','L',13.5,'class_2'],\n",
        "           ['blue','XL',15.3,'class_3']\n",
        "           ]\n",
        "df = pd.DataFrame(tshirts)\n",
        "df.columns = ['color', 'size','price','class']\n",
        "df.head()"
      ],
      "metadata": {
        "id": "N4r86RZD92g-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mapping the size feature**"
      ],
      "metadata": {
        "id": "LpDGYyc-BBtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "size_mapping = {'M': 1, 'L': 2,'XL':3}\n",
        "df['size'] = [size_mapping[item] for item in df['size']]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "k18d6faFBFO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoding Class Labels**"
      ],
      "metadata": {
        "id": "k-AhIBt_B_HQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_mapping = {feature_value: label for label,feature_value in enumerate(np.unique(df['class']))}\n",
        "class_mapping"
      ],
      "metadata": {
        "id": "g_TyqM-3BsFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['class'] = [class_mapping[item] for item in df['class']]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "h9sSqsz-CNV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ALternate approach to the above method One Hot Encoding**"
      ],
      "metadata": {
        "id": "JGULtA7OCrCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "class_labels = LabelEncoder()\n",
        "y = class_labels.fit_transform(df['class'].values)\n",
        "y"
      ],
      "metadata": {
        "id": "irzPLrn6CuKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.iloc[[0,1,2],:-1].values\n",
        "X"
      ],
      "metadata": {
        "id": "UqxnJzVPC6Xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "color_labels = LabelEncoder()\n",
        "# Perform encoding on color labels\n",
        "X[:,0] = color_labels.fit_transform(X[:,0])\n",
        "X"
      ],
      "metadata": {
        "id": "qUT2TirKDWHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Performing OneHotEncoding\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "X = df.iloc[[0,1,2],:-1].values\n",
        "column_transform = ColumnTransformer([\n",
        "    ('onehot', OneHotEncoder(),[0]),\n",
        "    ('nothing', 'passthrough',[1,2])\n",
        "])\n",
        "column_transform.fit_transform(X).astype(float)"
      ],
      "metadata": {
        "id": "gIkHOEOqVj26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Alternative way is to use the get_dummies method will\n",
        "only convert string columns and leave all other columns unchanged**"
      ],
      "metadata": {
        "id": "ra674HWpXvW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.get_dummies(df[['price','color','size']])\n",
        "pd.get_dummies(df[['price', 'color', 'size']],drop_first=True)"
      ],
      "metadata": {
        "id": "_rmOxgWlX0YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trying out ydata profiling for EDA"
      ],
      "metadata": {
        "id": "uaGU7CQjPltE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ydata-profiling"
      ],
      "metadata": {
        "id": "-3COulX9Qm2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ydata_profiling import ProfileReport"
      ],
      "metadata": {
        "id": "RQf8LsDMQHkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "iris = sns.load_dataset('iris')\n",
        "iris.head()"
      ],
      "metadata": {
        "id": "m1Ks4uQkPq1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profile = ProfileReport(iris, title=\"Profiling Report\")\n",
        "profile"
      ],
      "metadata": {
        "id": "b0nQMcz4QE0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Scaling"
      ],
      "metadata": {
        "id": "upuxP2rQc7i1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Decision\n",
        "trees and random forests are two of the very few machine learning algorithms where we **don’t need to\n",
        "worry about feature scaling**. Those algorithms are scale-invariant. However, the majority of machine\n",
        "learning and optimization algorithms behave much better if features are on the same scale,*"
      ],
      "metadata": {
        "id": "siTc02UzdBV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Two common approaches to bringing different features onto the same scale: normalization and standardization**\n",
        "\n",
        "*normalization refers to the rescaling of the features\n",
        "to a range of [0, 1], which is a special case of min-max scaling*"
      ],
      "metadata": {
        "id": "SP7nxoPXn5My"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing Standardization using `MinMaxScalar`**"
      ],
      "metadata": {
        "id": "pYwOW3gSqEBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = sns.load_dataset('iris')\n",
        "iris.head()"
      ],
      "metadata": {
        "id": "Papeg1bwoah7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = iris.iloc[:,[0,1,2,3]].values\n",
        "X"
      ],
      "metadata": {
        "id": "-A8Zkfa0oq4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = iris.iloc[:,-1].values"
      ],
      "metadata": {
        "id": "20HyRVBtovh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
      ],
      "metadata": {
        "id": "uNhgVUvroNT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "mns = MinMaxScaler()\n",
        "X_train_norm = mns.fit_transform(X_train)\n",
        "X_test_norm = mns.transform(X_test)"
      ],
      "metadata": {
        "id": "Lk1WMDTxdFdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Although normalization via min-max scaling is a commonly used technique that is useful when we\n",
        "need values in a bounded interval, standardization can be more practical for many machine learning\n",
        "algorithms, especially for optimization algorithms such as gradient descent. The reason is that many\n",
        "linear models, such as the logistic regression and SVM from Chapter 3, initialize the weights to 0 or\n",
        "small random values close to 0. Using standardization, we center the feature columns at mean 0 with\n",
        "standard deviation 1 so that the feature columns have the same parameters as a standard normal\n",
        "distribution (zero mean and unit variance), which makes it easier to learn the weights.*"
      ],
      "metadata": {
        "id": "ixu4RxRaplQT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing Normalization using `StandardScalar`**"
      ],
      "metadata": {
        "id": "pX75OhWApunZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "stdc = StandardScaler()\n",
        "X_train_std = stdc.fit_transform(X_train)\n",
        "X_test_std = stdc.transform(X_test)"
      ],
      "metadata": {
        "id": "X7pbDEI-pmlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Manually Applying the both methods*"
      ],
      "metadata": {
        "id": "vxNNeFueqgch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arr = np.array([0,1,2,3,4,5])\n",
        "std = np.array((arr - arr.mean())/ arr.std())\n",
        "nrm = np.array((arr - arr.min())/ arr.max() - arr.min())\n",
        "\n",
        "table = pd.DataFrame({'Standardization': std, 'Normalization': nrm})\n",
        "print(f\"Standardization {((arr - arr.mean())/ arr.std())}\")\n",
        "print(f\"Normalization {((arr - arr.min())/ arr.max() - arr.min())}\")"
      ],
      "metadata": {
        "id": "jW5G_9Z5qkOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table.head()"
      ],
      "metadata": {
        "id": "J1JL6A1arjpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Important to highlight that we fit the StandardScaler class only once—on the training\n",
        "data—and use those parameters to transform the test dataset or any new data point.\n",
        "Other, more advanced methods for feature scaling are available from scikit-learn, such **RobustScaler**.*\n",
        "\n",
        "\n",
        "*RobustScaler is especially helpful and recommended if we are working with small datasets that\n",
        "contain many outliers. Similarly, if the machine learning algorithm applied to this dataset is prone\n",
        "to overfitting, RobustScaler can be a good choice. Operating on each feature column independently,\n",
        "RobustScaler removes the median value and scales the dataset according to the 1st and 3rd quartile of\n",
        "the dataset (that is, the 25th and 75th quantile, respectively) such that more extreme values and outliers\n",
        "become less pronounced*"
      ],
      "metadata": {
        "id": "P5-kdUzmqgZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# L1 & L2 Regularization"
      ],
      "metadata": {
        "id": "XIPhSsme6fAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "\n",
        "wine = load_wine()\n",
        "\n",
        "df = pd.DataFrame(data = wine.data, columns = wine.feature_names)\n",
        "df['target'] = wine.target\n",
        "df.head()"
      ],
      "metadata": {
        "id": "MR66i4AQ7OmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.iloc[:,:-1].values\n",
        "y = df.iloc[:,-1].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3)"
      ],
      "metadata": {
        "id": "gxZfmh7i68Ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "stdc = StandardScaler()\n",
        "X_train_std = stdc.fit_transform(X_train)\n",
        "X_test_std = stdc.transform(X_test)"
      ],
      "metadata": {
        "id": "m2BGOSUg9Y4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applying L1 Regularization**\n",
        "\n",
        "*For regularized models in scikit-learn that support L1 regularization, we can simply set the penalty\n",
        "parameter to 'l1' to obtain a sparse solution:*\n",
        "\n",
        "**Note:** that we also need to select a different optimization algorithm (for example, solver='liblinear'),\n",
        "since 'lbfgs' currently does not support L1-regularized loss optimization*"
      ],
      "metadata": {
        "id": "wW_ybVlw8tPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(C=1.0,penalty='l1', solver='liblinear', multi_class='ovr')\n",
        "model.fit(X_train_std, y_train)\n",
        "print(f\"Training Accuracy {model.score(X_train_std,y_train)}\")"
      ],
      "metadata": {
        "id": "mGt-EHos8sns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.coef_"
      ],
      "metadata": {
        "id": "clI6yDSB9KSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = plt.subplot(111)\n",
        "colors = ['blue', 'green', 'red', 'cyan','magenta', 'yellow', 'black','pink', 'lightgreen', 'lightblue','gray', 'indigo', 'orange']\n",
        "\n",
        "weights, params = [], []\n",
        "for c in np.arange(-4., 6.):\n",
        "  model = LogisticRegression(penalty='l1', C=10.**c, solver='liblinear', multi_class='ovr')\n",
        "  model.fit(X_train_std, y_train)\n",
        "  weights.append(model.coef_[2])\n",
        "  params.append(10.**c)\n",
        "\n",
        "weights = np.array(weights)\n",
        "# column is the index\n",
        "for column, color in zip(range(weights.shape[1]), colors):\n",
        "  plt.plot(params, weights[:, column], label=df.columns[column],\n",
        "           color = color)\n",
        "plt.axhline(0, color='black', linestyle='--', linewidth=3)\n",
        "plt.xlim([10**(-5), 10**5])\n",
        "plt.ylabel('Weight coefficient')\n",
        "plt.xlabel('C (inverse regularization strength)')\n",
        "plt.xscale('log')\n",
        "plt.legend(loc='upper left')\n",
        "ax.legend(loc='upper center',bbox_to_anchor=(1.38, 1.03),ncol=1, fancybox=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3tio9mee_d4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The resulting plot provides us with further insights into the behavior of L1 regularization. As we can\n",
        "see, all feature weights will be zero if we penalize the model with a strong regularization parameter\n",
        "(C < 0.01); C is the inverse of the regularization parameter, 𝜆:**"
      ],
      "metadata": {
        "id": "R_jhaJWRBZVb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Selection Techniques"
      ],
      "metadata": {
        "id": "_f-Kyk8TQjAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Sequential Feature Algorithms***"
      ],
      "metadata": {
        "id": "4WiFOyCpRC-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Alternative way to reduce the complexity of the model and avoid overfitting is dimensionality\n",
        "reduction via feature selection, which is especially useful for unregularized models. There are two\n",
        "main categories of dimensionality reduction techniques: feature selection and feature extraction. Via\n",
        "feature selection, we select a subset of the original features, whereas in feature extraction, we derive\n",
        "information from the feature set to construct a new feature subspace.*\n",
        "\n",
        "* Sequential feature selection algorithms are a family of greedy search algorithms that are used to\n",
        "reduce an initial d-dimensional feature space to a k-dimensional feature subspace where k<d. The\n",
        "motivation behind feature selection algorithms is to automatically select a subset of features that are\n",
        "most relevant to the problem, to improve computational efficiency, or to reduce the generalization\n",
        "error of the model by removing irrelevant features or noise, which can be useful for algorithms that\n",
        "don’t support regularization\n",
        "\n",
        "* A classic sequential feature selection algorithm is sequential backward selection (SBS), which aims to\n",
        "reduce the dimensionality of the initial feature subspace with a minimum decay in the performance\n",
        "of the classifier to improve upon computational efficiency. The idea behind the SBS algorithm is quite simple: SBS sequentially removes features from the full\n",
        "feature subset until the new feature subspace contains the desired number of feature"
      ],
      "metadata": {
        "id": "ECXW92AqQmMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlxtend"
      ],
      "metadata": {
        "id": "Iit1TtKHQl0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wine_data = load_wine()\n",
        "df = pd.DataFrame(data = wine_data.data, columns = wine_data.feature_names)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "oR2UsbTnR2Xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Target'] = wine_data.target\n",
        "df.head()"
      ],
      "metadata": {
        "id": "2BZ21bLwSH7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Class Labels {np.unique(df['Target'])}\")"
      ],
      "metadata": {
        "id": "C5XOLpOfSLkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Scaling the data as KNN is sensitive to Scaling*"
      ],
      "metadata": {
        "id": "TIAbjWn0W5M0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
        "\n",
        "sc = StandardScaler()\n",
        "X_train_std = sc.fit_transform(X_train)\n",
        "X_test_std = sc.transform(X_test)"
      ],
      "metadata": {
        "id": "77RXu1yZSacS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Build a dummy KNN Model*"
      ],
      "metadata": {
        "id": "bg8lsvTcSvde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "model = KNeighborsClassifier(n_neighbors=5)\n",
        "model.fit(X_train_std, y_train)\n",
        "print(f\"Training Accuracy {model.score(X_train_std, y_train)}\")\n",
        "print(f\"Testing Accuracy {model.score(X_test_std, y_test)}\")"
      ],
      "metadata": {
        "id": "STbqrMqLSuE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Selecting Best 5 Features**\n",
        "\n",
        "* `forward=True` & `floating=False` indicates it is SFS\n",
        "* `forward=False` & `floating=False` indicates it is SBS"
      ],
      "metadata": {
        "id": "k7mHfHUxTRpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "\n",
        "sfs_1 = SFS(model, k_features=5,forward=True,floating=False,verbose=2,scoring='accuracy',cv=5)\n",
        "sfs_1 = sfs_1.fit(X_train_std, y_train)"
      ],
      "metadata": {
        "id": "l5yQLeMxTOSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get the best feature Index using `k_feature_idx_`**"
      ],
      "metadata": {
        "id": "Gum5MVfdTzeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sfs_1.k_feature_idx_"
      ],
      "metadata": {
        "id": "_8-jvYbdTv1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get the best features**"
      ],
      "metadata": {
        "id": "CvxmRbiHUauC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns[:][list(sfs_1.k_feature_idx_)]"
      ],
      "metadata": {
        "id": "GZzIZt8FT6tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extracting data based on the best features using `transform`**"
      ],
      "metadata": {
        "id": "WeRDry0KUjMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_selected = sfs_1.transform(X_train_std)\n",
        "X_test_selected = sfs_1.transform(X_test_std)\n",
        "\n",
        "model.fit(X_train_selected, y_train)"
      ],
      "metadata": {
        "id": "cAgUF4sUUgeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training Accuracy {model.score(X_train_selected, y_train)}\")\n",
        "print(f\"Testing Accuracy {model.score(X_test_selected, y_test)}\")"
      ],
      "metadata": {
        "id": "oIHeHymqU7N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We can see from the graph that with 5 features we get an accuracy of nearly 99%**"
      ],
      "metadata": {
        "id": "lI1az0oBWIRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
        "metric_dict = sfs_1.get_metric_dict(confidence_interval=0.95)\n",
        "fig1 = plot_sfs(metric_dict, kind='std_dev')"
      ],
      "metadata": {
        "id": "6Q4T5gJPVdG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Selecting Best Features without providing any size/num of features we need**\n",
        "\n",
        "*Set `k_features='best'`*"
      ],
      "metadata": {
        "id": "CF_5TFRHVT16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sfs_2 = SFS(model, k_features='best',forward=True,floating=False,verbose=2,scoring='accuracy',cv=5)\n",
        "sfs_2 = sfs_2.fit(X_train_std, y_train)"
      ],
      "metadata": {
        "id": "XRn_34lZVbEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training Accuracy {model.score(X_train_selected, y_train)}\")\n",
        "print(f\"Testing Accuracy {model.score(X_test_selected, y_test)}\")"
      ],
      "metadata": {
        "id": "gq_--M1VWgMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So, using 9 features is probably the best idea to get maximum accuracy**"
      ],
      "metadata": {
        "id": "Btb9eIZaWpxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric_dict = sfs_2.get_metric_dict(confidence_interval=0.95)\n",
        "fig1 = plot_sfs(metric_dict, kind='std_dev')"
      ],
      "metadata": {
        "id": "ZsaT_PopWkzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assessing feature importance with random forests"
      ],
      "metadata": {
        "id": "1dP3wJ4YciKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*We learnt how to use L1 regularization to zero out irrelevant features via logistic\n",
        "regression and how to use the SBS algorithm for feature selection and apply it to a KNN algorithm.\n",
        "Another useful approach for selecting relevant features from a dataset is using a random forest, an en-\n",
        "semble technique*"
      ],
      "metadata": {
        "id": "WlFqEfc3clIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*we can measure the feature\n",
        "importance as the averaged impurity decrease computed from all decision trees in the forest, without\n",
        "making any assumptions about whether our data is linearly separable or not. Conveniently, the random\n",
        "forest implementation in scikit-learn already collects the feature importance values for us so that we\n",
        "can access them via the `feature_importances_` attribute after fitting a `RandomForestClassifier`*"
      ],
      "metadata": {
        "id": "G2ZJWUhYeoOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.argsort(importances[::-1])"
      ],
      "metadata": {
        "id": "JsYJeqnKdsHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "feature_labels = df.columns[:-1]\n",
        "forest = RandomForestClassifier(n_estimators=500)\n",
        "forest.fit(X_train, y_train)\n",
        "importances = forest.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "# for f in range(X_train.shape[1]):\n",
        "#   print(\"%2d) %-*s %f\" % (f + 1, 30,feature_labels[indices[f]],importances[indices[f]]))\n",
        "plt.title('Feature importance')\n",
        "plt.bar(range(X_train.shape[1]),\n",
        "importances[indices],\n",
        "align='center')\n",
        "plt.xticks(range(X_train.shape[1]),\n",
        "feature_labels[indices], rotation=90)\n",
        "plt.xlim([-1, X_train.shape[1]])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ns_tRnQxcr6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We can conclude that the proline and flavonoid levels, the color intensity, the OD280/OD315 diffraction,\n",
        "and the alcohol concentration of wine are the most discriminative features in the dataset based on\n",
        "the average impurity decrease in the 500 decision trees**"
      ],
      "metadata": {
        "id": "iwKFGY5ieIZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Getting the data we could set the threshold to 0.1 to\n",
        "reduce the dataset to the five most important feature*"
      ],
      "metadata": {
        "id": "wdw2JwH-fGVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectFromModel\n",
        "sfm = SelectFromModel(forest, threshold=0.1, prefit=True)\n",
        "X_selected = sfm.transform(X_train)\n",
        "print('Number of features that meet this threshold','criterion:', X_selected.shape[1])\n",
        "for f in range(X_selected.shape[1]):\n",
        "  print(\"%2d) %-*s %f\" % (f + 1,30,feature_labels[indices[f]],importances[indices[f]]))"
      ],
      "metadata": {
        "id": "Eog1oL97eLLs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}